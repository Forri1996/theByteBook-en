# 4.3.1 四层负载均衡工作模式

四层负载均衡器最典型的软件实现是 LVS（Linux Virtual Server，Linux 虚拟服务器），但笔者并不过多介绍 LVS 的相关特点和如何使用，而是从网络数据包转发的角度去解释 LVS 中 DR、NAT、Tunnel 模式是如何工作的。

## 1. 链路层负载均衡

LVS 的 DR 模式其实是一种链路层负载均衡，数据链路层负载均衡所做的工作，就是修改请求的数据帧中的 MAC 目标地址，让原本发送给负载均衡器请求的数据帧，被二层交换机转发至服务器集群中对应的服务器，这样真实的服务器就获得了一个原本目标并不是发送给它的数据帧。由于链路层只修改了 MAC 地址，所以在 IP 层看来数据没有任何变化，继而被正常接收。由于 IP 数据包中包含了源（客户端）和目的地（负载均衡器）IP 地址，只有真实服务器保证自己的 IP 与 数据包中的目的地 IP 地址一致，这个数据包才能继续被处理。

因此使用这种负载均衡模式时，需要把真实服务器的 VIP 配置成和负载均衡器的 VIP 一样，LVS 配置真实服务器的原理也是如此，如下示例。

```plain
// 真实服务器配置
ifconfig lo:0 1.1.1.1 netmask 255.255.255.255 up // 绑定 VIP 地址在 lo 接口上才能收到目标地址为 VIP 的包。
route add -host 1.1.1.1 dev lo  //  目标地址是 VIP 的数据包从本机 lo 接口发送出去
```

只有请求经过负载均衡器，而服务的响应无需从负载均衡器原路返回的工作模式中，整个请求、转发、响应的链路形成一个“三角关系”，所以这种模式也被形象的称为“三角传输模式”（Direct Server Return，DSR），也称为“单臂模式”（Single Legged Model），如图 4-2 所示。

<div  align="center">
	<img src="../assets/balancer4-dsr.svg" width = "550"  align=center />
	<p>图4-7 链路层 DSA 模式负载均衡</p>
</div>

设计 DSR 的主要原因是：在一些场景中，响应的流量要远远大于请求的流量（例如典型的 HTTP request/response 模式）。假设请求占 10% 的流量，响应占 90%，使用 DSR 技术，只需 1/10 的带宽就可以满足系统需求，这种类型的优化可以极大地节省成本，还提高了负载均衡器的可靠性（流量越低肯定越好）。

虽然 DSR 模式效率很高，但它在缺陷也很明显，因为响应不再经过负载均衡器，负载均衡器就无法知道 TCP 连接的完整状态，防火墙策略会受到影响（TCP 连接只有 SYN，没有 ACK）。其次因为是数据链路层通信，受到的网络侧约束也很大。所以优势（效率高）和劣势（不能跨子网）共同决定了 DSR 模式最适合作为数据中心的第一层均衡设备，用来连接其他的下级负载均衡器。


## 2. 网络层负载均衡

网络层负载均衡操作的是 IP 数据包，我们先对 IP 数据包有个简单的了解。以 IP 协议为例，一个 IP 数据包由头部（Header）和载荷（Payload）组成，Header 内部包含版本、源地址、目的地地址等信息，结构如图所示。

<div  align="center">
	<img src="../assets/ip.svg" width = "350"  align=center />
	<p>图4-8 ip 数据包结构</p>
</div>

在本文，我们无需太关注 Header 头部信息，只要知道 Header 内有源地址（Source address）和目的地地址（Destination address）即可。既然数据链层负载均衡可以通过改写 MAC 地址来实现数据包转发，到了网络层，我们也可以继续沿用改写 MAC 相似的思路修改网络层的 IP 数据包地址信息来实现数据转发。

LVS 的 Tunnel、NAT 模式都属于网络层负载均衡，只不过因为对 IP 数据包的不同形式修改而分成两种，我们先来看第一种修改方式。

### 2.1 IP 隧道模式

第一种保持源数据包不变，新建一个 IP 数据包，将原来的 IP 数据包整体放进新 IP 数据包的 Payload 内，再通过三层交换机发送出去，真实服务器收到包之后，有一个对应拆包的机制，把负载均衡器自动添加的那层 Header 删掉，解析出 Payload 内部的 IP 数据包再进行正常处理。把一个数据包封装在另一个数据包内，其实就是一种隧道技术，比如 Linux 中的隧道技术 ipip 就是字面 IP in IP 的意思。由于 IP 隧道工作在网络层，因此摆脱了直接路由模式的网络约束，所以 LVS Tunnel 模式可以跨越 VLAN 。

由于没有修改源数据包的任何信息，所以 IP 隧道模式仍具有三角传输模式的特点，即负载均衡转发器转发进来的请求，真实服务器去响应请求，IP 隧道模式从请求到响应的过程如图所示。

<div  align="center">
	<img src="../assets/balancer4-tunnel.svg" width = "550"  align=center />
	<p>图4-9 tunnel 模式</p>
</div>

IP 隧道模式相当于 DR 模式的升级（支持了跨网），不过由于使用隧道模式，所以要求真实服务器支持隧道协议，真实服务器只局限在部分 Linux 系统上（不过笔者也没见过四层负载均衡使用非 Linux 系统的案例）；其次，只要是三角模式（LVS 的 DR 模式或者 Tunnel 模式）必须要保证真实服务器与负载均衡服务器有相同的虚拟 IP 地址，因为回复客户端时，必须使用这个虚拟的 IP 作为数据包的源地址，这样客户端收到数据包之后才能正常解析；最后，因为真实服务器和客户端对接，所以真实服务器得能访问外网。


### 2.2 NAT 模式

IP 数据包的另外一种改写方式是直接改变 IP 数据包的 Header 内的目的地（Destination address）地址（改为真实服务器的地址），修改之后原本用户发送给负载均衡器的数据包会被三层交换机转发至真实服务器的网卡上。这么解释似乎不太好理解，但相信每一个读者都配置过这种原理的操作（配置路由器），这种模式和一台路由器带着一群内网机器上网的“网络地址转换”（Network Address Translation）一个原理。因此，这种负载均衡的模式被称为 NAT 模式，此模式请求到响应的过程如图所示。

<div  align="center">
	<img src="../assets/balancer4-NAT.svg" width = "550"  align=center />
	<p>图4-10 NAT 模式负载均衡</p>
</div>

这种类型中，TCP 连接在负载均衡器建立连接跟踪和网络地址转换（NAT）之后直接转发给选中的后端。例如，假设客户端正在和负载均衡器 1.1.1.1:80 通信，选中的后端是 10.0.0.2:8080。当客户端的 TCP 包到达负载均衡器时，负载均衡器会将包的目的 IP/port （从 1.1.1.1:80）换成 10.0.0.2:8080，以及将源 IP/port 换成负载均衡器自己的 IP/port。当应答包回来的时候，负载均衡器再做相反的转换。

不过 NAT 模式的缺陷也显而易见，因为负载均衡器代表整个服务集群接收/应答，当流量压力比较大的时候，整个系统的瓶颈就很容易出现在负载均衡器上。
